{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex as re\n",
    "from underthesea import word_tokenize\n",
    "import pymongo\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = \"mongodb://chatbot:BmPspGbuVgdG@103.113.83.201:27017/?authSource=ChatBotDB&readPreference=primary\"\n",
    "DBNAME = \"ChatBotDB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_mongodb():\n",
    "    try:\n",
    "        database = pymongo.MongoClient(URI)\n",
    "        db = database[DBNAME]\n",
    "        return db\n",
    "    except Exception as exc:\n",
    "        print(\"Error in: \", exc)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = connect_mongodb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    " \n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    " \n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    " \n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    " \n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "        else:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    return ''.join(chars)\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'Vật liệu thép dùng làm gối cầu metro số 1 không đúng theo hợp đồng ban đầu khiến chủ đầu tư lo ngại chất lượng và tuổi thọ công trình bị ảnh hưởng nghiêm trọng.Liên quan đến việc điều tra sự cố metro số 1 (Bến Thành - Suối Tiên), ngày 22/1, nguồn tin của Zing cho biết vật liệu thép sử dụng cho tất cả gối cầu được xác định không đạt yêu cầu theo quy định hợp đồng ký năm 2012. Các hạng mục này thuộc gói thầu CP2 (đoạn trên cao và depot) dự án metro số 1, do liên danh SCC (Sumitomo - Cienco 6) làm tổng thầu.Phía nhà thầu SCC đã nhận được các hồ sơ liên quan do Ban quản lý đường sắt đô thị TP.HCM (MAUR) gửi đi. Kèm theo khiếu nại, MAUR cho rằng hành động trên tác động nghiêm trọng đến chất lượng và tuổi thọ công trình.\"Cụ thể, theo hợp đồng, liên danh SCC phải sử dụng vật liệu bằng hoặc cao hơn tiêu chuẩn thiết kế đưa ra. Tuy nhiên, SCC đã dùng vật liệu thép làm gối cầu khác tiêu chuẩn đã liệt kê, dẫn đến thép không đạt giới hạn chảy như yêu cầu\", nguồn tin cho hay.Vị trí gối cầu bị xê dịch 5 cm gần đây (đoạn Ngã tư Thủ Đức - Bình Thái). Ảnh: T.T.Trong khi metro số 1 đang trong giai đoạn gấp rút hoàn thành để đưa vào khai thác cuối năm nay, dự án này liên tiếp phát hiện các sự cố trên công trình.Tháng 10/2020, MAUR phát hiện vụ trượt gối cao su khỏi đá kê đầu tiên tại trụ  P14-10 (đoạn ga Công nghệ cao, hướng Bến Thành đi Suối Tiên). Sự cố đang trong quá trình đánh giá nguyên nhân thì mới đây, tháng 1/2021, một gối cầu khác (đoạn ngã tư Thủ Đức và Bình Thái) được phát hiện trong tình trạng xê dịch vị trí.Hội đồng chuyên gia nhận định sự cố có thể không phải hiện tượng đơn lẻ. Đến nay, các bước thí nghiệm, quan trắc chất lượng, kỹ thuật gối vẫn đang diễn ra.Liên quan sự việc này, Thứ trưởng Bộ Xây dựng Lê Quang Hùng, Cục trưởng Cục Giám định Phạm Minh Hà cùng hội đồng khoa học đã có 2 ngày làm việc tại TP.HCM nhằm đẩy nhanh tiến trình điều tra vụ việc.Phía Ban Quản lý đường sắt đô thị TP.HCM yêu cầu nhà thầu và đơn vị tư vấn giải thích thỏa đáng. Còn nhà thầu SCC cho rằng cuối tháng 1 mới có thể hoàn tất các hồ sơ giải trình liên quan.'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(document):\n",
    "    document = convert_unicode(document)\n",
    "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    document = document.lower()\n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    document = convert_unicode(document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chuanhoa = text_preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stopword\n",
    "stop_word = set()\n",
    "\n",
    "f = open(\"./stop_word.txt\", \"r\")\n",
    "data = f.read()\n",
    "f.close()\n",
    "data = convert_unicode(data)\n",
    "list_stop_word = data.split(\"\\n\")\n",
    "for sw in list_stop_word:\n",
    "    stop_word.add(sw)\n",
    " \n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stop_word and not word.isnumeric():\n",
    "            words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_categories_index = {}\n",
    "categories = list(db[\"news\"].find({}, {\"_id\" : 0, \"Category\" : 1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    if \"Category\" in category and category[\"Category\"] != \"\" and len (category[\"Category\"].split(\" \")) < 3:\n",
    "        chuanhoa = chuan_hoa_dau_cau_tieng_viet(category[\"Category\"])\n",
    "        if chuanhoa not in dict_categories_index:\n",
    "            dict_categories_index[chuanhoa] = []\n",
    "        if category[\"Category\"] not in dict_categories_index[chuanhoa]:\n",
    "            dict_categories_index[chuanhoa].append(category[\"Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]thế giới\n",
      "100%|██████████| 2/2 [04:36<00:00, 138.00s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]chính trị\n",
      "100%|██████████| 1/1 [03:18<00:00, 198.73s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]sao việt\n",
      "100%|██████████| 1/1 [01:40<00:00, 101.00s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]trang chủ\n",
      "100%|██████████| 2/2 [00:19<00:00,  9.75s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]châu mỹ\n",
      "100%|██████████| 1/1 [00:44<00:00, 44.75s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tai nghe\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]thời sự\n",
      "100%|██████████| 1/1 [02:09<00:00, 129.50s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]pháp luật\n",
      "100%|██████████| 2/2 [02:46<00:00, 83.16s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]xã hội\n",
      "100%|██████████| 2/2 [04:04<00:00, 122.23s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]thể thao\n",
      "100%|██████████| 1/1 [01:17<00:00, 77.25s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]khoa học\n",
      "100%|██████████| 1/1 [01:42<00:00, 102.65s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]sức khỏe\n",
      "100%|██████████| 1/1 [02:10<00:00, 130.99s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]điểm nóng\n",
      "100%|██████████| 1/1 [00:49<00:00, 49.38s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]giải trí\n",
      "100%|██████████| 1/1 [02:34<00:00, 154.13s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]bóng đá\n",
      "100%|██████████| 1/1 [01:12<00:00, 72.97s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]giao thông\n",
      "100%|██████████| 1/1 [01:30<00:00, 90.99s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tv show\n",
      "100%|██████████| 1/1 [01:56<00:00, 116.23s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]việc làm\n",
      "100%|██████████| 1/1 [01:08<00:00, 68.30s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]xe ++\n",
      "100%|██████████| 1/1 [00:28<00:00, 28.60s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]công nghệ\n",
      "100%|██████████| 1/1 [02:27<00:00, 147.64s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]xuất bản\n",
      "100%|██████████| 1/1 [02:56<00:00, 176.22s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]kinh doanh\n",
      "100%|██████████| 2/2 [06:16<00:00, 188.10s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tác giả\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.23s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]văn hóa\n",
      "100%|██████████| 1/1 [02:46<00:00, 166.92s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]sách\n",
      "100%|██████████| 1/1 [04:22<00:00, 262.26s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]đời sống\n",
      "100%|██████████| 1/1 [02:43<00:00, 163.56s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]âm nhạc\n",
      "100%|██████████| 1/1 [02:54<00:00, 174.85s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]số hóa\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.96s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]xe\n",
      "100%|██████████| 1/1 [01:34<00:00, 94.62s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]phân tích\n",
      "100%|██████████| 1/1 [05:09<00:00, 309.70s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]star style\n",
      "100%|██████████| 1/1 [00:31<00:00, 31.55s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]học đường\n",
      "100%|██████████| 1/1 [02:55<00:00, 175.84s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]xe máy\n",
      "100%|██████████| 1/1 [01:35<00:00, 95.76s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]esports\n",
      "100%|██████████| 1/1 [00:25<00:00, 25.65s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]hậu trường\n",
      "100%|██████████| 2/2 [02:07<00:00, 63.79s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]sự kiện\n",
      "100%|██████████| 1/1 [02:10<00:00, 130.33s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]việt nam\n",
      "100%|██████████| 1/1 [02:30<00:00, 150.18s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]khỏe đẹp\n",
      "100%|██████████| 1/1 [02:00<00:00, 120.46s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]cao thủ\n",
      "100%|██████████| 1/1 [01:37<00:00, 97.39s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]thời trang\n",
      "100%|██████████| 1/1 [01:51<00:00, 111.84s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]du lịch\n",
      "100%|██████████| 2/2 [02:59<00:00, 89.64s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tài chính\n",
      "100%|██████████| 1/1 [02:52<00:00, 172.98s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]mobile\n",
      "100%|██████████| 1/1 [01:58<00:00, 118.21s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]giáo dục\n",
      "100%|██████████| 1/1 [02:23<00:00, 143.69s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tôi viết\n",
      "100%|██████████| 1/1 [01:59<00:00, 119.12s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]bạn đọc\n",
      "100%|██████████| 1/1 [02:39<00:00, 159.53s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]ôtô\n",
      "100%|██████████| 1/1 [02:16<00:00, 137.00s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]phim ảnh\n",
      "100%|██████████| 1/1 [02:56<00:00, 176.79s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]quốc tế\n",
      "100%|██████████| 1/1 [00:54<00:00, 54.92s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]dị\n",
      "100%|██████████| 1/1 [00:24<00:00, 24.12s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]hoàn cảnh\n",
      "100%|██████████| 1/1 [01:33<00:00, 93.97s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]blog\n",
      "100%|██████████| 1/1 [01:01<00:00, 61.45s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]internet\n",
      "100%|██████████| 1/1 [01:00<00:00, 60.21s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]châu á\n",
      "100%|██████████| 1/1 [02:33<00:00, 153.40s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]star\n",
      "100%|██████████| 1/1 [02:09<00:00, 129.85s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tư liệu\n",
      "100%|██████████| 1/1 [03:46<00:00, 226.44s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]du học\n",
      "100%|██████████| 2/2 [03:05<00:00, 92.65s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]pháp đình\n",
      "100%|██████████| 1/1 [02:25<00:00, 145.83s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]xu hướng\n",
      "100%|██████████| 1/1 [01:42<00:00, 102.63s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]môi trường\n",
      "100%|██████████| 1/1 [00:50<00:00, 50.75s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]giới trẻ\n",
      "100%|██████████| 2/2 [03:13<00:00, 96.88s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]thị trường\n",
      "100%|██████████| 1/1 [01:56<00:00, 116.37s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]nhà đất\n",
      "100%|██████████| 1/1 [00:34<00:00, 34.20s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]làm đẹp\n",
      "100%|██████████| 3/3 [01:41<00:00, 33.97s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]ẩm thực\n",
      "100%|██████████| 1/1 [01:35<00:00, 95.08s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]2-tek\n",
      "100%|██████████| 1/1 [01:54<00:00, 114.44s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]xem-ăn-chơi\n",
      "100%|██████████| 1/1 [02:01<00:00, 121.10s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]doanh nghiệp\n",
      "100%|██████████| 1/1 [00:57<00:00, 57.48s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]đẹp\n",
      "100%|██████████| 1/1 [00:21<00:00, 21.69s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]võ thuật\n",
      "100%|██████████| 1/1 [01:24<00:00, 84.15s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]gương sáng\n",
      "100%|██████████| 1/1 [00:51<00:00, 51.73s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]nhật ký\n",
      "100%|██████████| 1/1 [00:30<00:00, 30.13s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]esport\n",
      "100%|██████████| 1/1 [01:11<00:00, 71.72s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tin tức\n",
      "100%|██████████| 1/1 [00:46<00:00, 46.84s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tuyển sinh\n",
      "100%|██████████| 1/1 [01:43<00:00, 103.39s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]concept\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]khám phá\n",
      "100%|██████████| 2/2 [00:47<00:00, 23.74s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]nhân vật\n",
      "100%|██████████| 1/1 [00:19<00:00, 19.62s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]khuyến học\n",
      "100%|██████████| 1/1 [00:39<00:00, 39.94s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]nhân ái\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.31s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]điện ảnh\n",
      "100%|██████████| 1/1 [01:00<00:00, 60.61s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]chính sách\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.48s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]người tốt\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]sống xanh\n",
      "100%|██████████| 1/1 [00:39<00:00, 39.25s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]dự án\n",
      "100%|██████████| 1/1 [00:38<00:00, 38.47s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]sport\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.10s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]an sinh\n",
      "100%|██████████| 1/1 [00:38<00:00, 38.30s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tình yêu\n",
      "100%|██████████| 1/1 [00:55<00:00, 55.48s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]ung thư\n",
      "100%|██████████| 1/1 [00:56<00:00, 56.13s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]chùm ảnh\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.36s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]ciné\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.32s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]đấu trường\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.03s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]âu-mỹ\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.27s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]musik\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.14s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]màn hình\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]phim\n",
      "100%|██████████| 1/1 [00:24<00:00, 24.63s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]yan sitcom\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.05s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]kiều bào\n",
      "100%|██████████| 1/1 [00:50<00:00, 50.85s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]loa\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]hồ sơ\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.01s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]đô thị\n",
      "100%|██████████| 1/1 [01:58<00:00, 118.77s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]cần biết\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.34s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]media\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]podcast\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]chất\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.75s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]vụ án\n",
      "100%|██████████| 1/1 [01:31<00:00, 91.35s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]chuyện lạ\n",
      "100%|██████████| 1/1 [01:19<00:00, 79.85s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]quân sự\n",
      "100%|██████████| 1/1 [02:11<00:00, 131.21s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tư vấn\n",
      "100%|██████████| 1/1 [01:54<00:00, 114.20s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tennis\n",
      "100%|██████████| 1/1 [00:45<00:00, 45.75s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]hàng không\n",
      "100%|██████████| 1/1 [01:21<00:00, 81.30s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]xe đạp\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tiêu dùng\n",
      "100%|██████████| 1/1 [02:26<00:00, 146.58s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]mobile audio\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]đào tạo\n",
      "100%|██████████| 1/1 [00:56<00:00, 56.08s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]thông báo\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]game\n",
      "100%|██████████| 1/1 [01:11<00:00, 71.50s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]bệnh gan\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.47s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]analog\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]dinh dưỡng\n",
      "100%|██████████| 1/1 [01:29<00:00, 89.22s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]meego\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]samsung bada\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]sao\n",
      "100%|██████████| 1/1 [02:51<00:00, 171.46s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]khởi nghiệp\n",
      "100%|██████████| 1/1 [01:05<00:00, 65.50s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]bảo hiểm\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.13s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]gps\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]computer audio\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]sống đẹp\n",
      "100%|██████████| 1/1 [01:09<00:00, 69.05s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]car audio\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]ý kiến\n",
      "100%|██████████| 1/1 [03:17<00:00, 197.20s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]đua xe\n",
      "100%|██████████| 1/1 [00:51<00:00, 51.24s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]thiên văn\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.19s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]tâm sự\n",
      "100%|██████████| 1/1 [01:39<00:00, 99.45s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]phượt\n",
      "100%|██████████| 1/1 [01:18<00:00, 78.70s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]nghe nhìn\n",
      "100%|██████████| 1/1 [00:31<00:00, 31.99s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]đánh giá\n",
      "100%|██████████| 1/1 [00:33<00:00, 33.39s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]văn học\n",
      "100%|██████████| 1/1 [00:57<00:00, 57.33s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]teen đẹp\n",
      "100%|██████████| 1/1 [00:25<00:00, 25.17s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]hollywood\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.79s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]diễn đàn\n",
      "100%|██████████| 1/1 [00:55<00:00, 55.91s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]multimedia\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]talks\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create data for category\n",
    "for key, categories in dict_categories_index.items():\n",
    "    print(key)\n",
    "    for category in tqdm(categories):\n",
    "        data = list(db[\"news\"].find({\"Category\" : category}, {\"_id\" : 0, \"Content\" : 1, \"Category\" : 1}).limit(2000))\n",
    "        result = count_word(data, key)\n",
    "        if result != \"\":\n",
    "            with open(\"./data_set.prep\", \"a\") as f:\n",
    "                f.write(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_word(data, category):\n",
    "    result = \"\"\n",
    "    dict_key_word = {}\n",
    "    for row in data:\n",
    "        content = text_preprocess(row[\"Content\"])\n",
    "        content = remove_stopwords(content)\n",
    "        list_word = word_tokenize(content)\n",
    "        for word in list_word:\n",
    "            word = convert_unicode(word)\n",
    "            if word not in dict_key_word:\n",
    "                dict_key_word[word] = 1\n",
    "            else:\n",
    "                dict_key_word[word] += 1\n",
    "        # dict_key_word = {k: v for k, v in sorted(dict_key_word.items(), key=lambda item: item[1], reverse=True)}\n",
    "        list_final_word = []\n",
    "        for k, v in dict_key_word.items():\n",
    "            if v > 300:\n",
    "                list_final_word.append(k)\n",
    "        result = \"__label__\" + text_preprocess(category).replace(\" \", \"_\") + \" \" + \" \".join(list_final_word) + \"\\n\"\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['__label__chính_trị', '__label__pháp_luật', '__label__sao_việt', '__label__thế_giới', '__label__thời_sự', '__label__xã_hội'] \n\n"
     ]
    }
   ],
   "source": [
    "# Chia tập dữ liệu để train\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "test_percent = 0.2\n",
    " \n",
    "text = []\n",
    "label = []\n",
    " \n",
    "for line in open('./data_set.prep'):\n",
    "    words = line.strip().split()\n",
    "    label.append(words[0])\n",
    "    text.append(words[1:])\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=test_percent, random_state=84)\n",
    "# Lưu train/test data\n",
    "# Giữ nguyên train/test để về sau so sánh các mô hình cho công bằng\n",
    "with open('train.txt', 'w') as fp:\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        fp.write('{} {}\\n'.format(y, x))\n",
    " \n",
    "with open('test.txt', 'w') as fp:\n",
    "    for x, y in zip(X_test, y_test):\n",
    "        fp.write('{} {}\\n'.format(y, x))\n",
    " \n",
    "# encode label\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(label)\n",
    "y_train = label_encoder.transform(y_train)\n",
    "print(list(label_encoder.classes_), '\\n')\n",
    "y_test = label_encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 3, 5, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cfc698ddddb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                 max_iter=10000))\n\u001b[1;32m     15\u001b[0m                     ])\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtext_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[1;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression(solver='lbfgs', \n",
    "                                                multi_class='auto',\n",
    "                                                max_iter=10000))\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    " \n",
    "train_time = time.time() - start_time\n",
    "print('Done training Linear Classifier in', train_time, 'seconds.')\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(\"./linear_classifier.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done training SVM in 0.024191617965698242 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC(gamma='scale'))\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    " \n",
    "train_time = time.time() - start_time\n",
    "print('Done training SVM in', train_time, 'seconds.')\n",
    " \n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(\"./svm.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear Classifier, Accuracy = 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = pickle.load(open(os.path.join(\"./linear_classifier.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('Linear Classifier, Accuracy =', np.mean(y_pred == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([86, 29, 29, 29, 29, 86, 29, 86, 29, 86, 29, 86, 86, 86, 29, 29, 86,\n",
       "        86, 29, 29, 29, 29]),\n",
       " array([ 38, 104,  30,  82,  61,  13,  24,  46,  10,  49,  39,  18,  32,\n",
       "         78,  91,  74,  76,  87,  34,  53,  45,  65]))"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('sofm suning thi_đấu vô_địch thế_giới lpl đội_hình team giải đấu tuyển_thủ chính_thức hợp_đồng rừng việt nam quyết_định áo mùa zing chuyên_nghiệp ảnh trận esports đi flash phantom liên_quân đấu_trường đối_đầu chức khả_năng lối tướng góp_mặt đường sát_thương giành chiến_thắng đồng_đội pha bảng lợi_thế đội vị màn mobile chung_kết kỹ_năng đội_tuyển sở_hữu xuân tiền hai diễn game đương_kim giao_tranh thua đại_diện tuần vòng á_quân thất_bại ván sn gaming đầu khu_vực phút kiểm_soát trụ rồng bùa lợi baron thành_viên liên_tiếp giúp giai_đoạn cạnh t1 hàn quốc huyền_thoại liên_minh kim damwon lee lck hè cktg thể_hiện top hạ gục xạ_thủ chỉ_số tổ_chức',\n",
       " '__label__esports')"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "text[29], label[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('__label__kinh_doanh', '__label__công_nghệ')"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "(label[12], label[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\""
   ]
  }
 ]
}